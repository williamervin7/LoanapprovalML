{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":325031,"sourceType":"datasetVersion","datasetId":137197}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üè¶ Loan Approval Prediction: Logistic Regression & Model Optimization\n\nThis notebook explores a classification problem using a loan approval dataset. The goal is to build a model that predicts whether a loan application will be approved based on applicant information such as income, marital status, credit history, and other features.\n\nWe start with exploratory data analysis and preprocessing, then build a baseline logistic regression model using all available features. From there, we refine the model through feature importance analysis, threshold tuning, and performance evaluation using metrics like precision, recall, and confusion matrices.\n\nAlong the way, we also compare the logistic regression model against ensemble models like Random Forest, Gradient Boosting, XGBoost, and LGBM to check if they offer meaningful improvements.\n\nThe focus is not just on accuracy but also on model interpretability, reducing false approvals, and balancing performance across classes.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:32.063796Z","iopub.execute_input":"2025-05-24T19:25:32.064103Z","iopub.status.idle":"2025-05-24T19:25:34.749591Z","shell.execute_reply.started":"2025-05-24T19:25:32.064078Z","shell.execute_reply":"2025-05-24T19:25:34.748282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train =  pd.read_csv('/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:34.751235Z","iopub.execute_input":"2025-05-24T19:25:34.751791Z","iopub.status.idle":"2025-05-24T19:25:34.784198Z","shell.execute_reply.started":"2025-05-24T19:25:34.751754Z","shell.execute_reply":"2025-05-24T19:25:34.783029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check For NAN values","metadata":{}},{"cell_type":"code","source":"df = train.copy()\ndf.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:34.787141Z","iopub.execute_input":"2025-05-24T19:25:34.787488Z","iopub.status.idle":"2025-05-24T19:25:34.821584Z","shell.execute_reply.started":"2025-05-24T19:25:34.787462Z","shell.execute_reply":"2025-05-24T19:25:34.820716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for duplicated values\ndf.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:34.822462Z","iopub.execute_input":"2025-05-24T19:25:34.822772Z","iopub.status.idle":"2025-05-24T19:25:34.838403Z","shell.execute_reply.started":"2025-05-24T19:25:34.822747Z","shell.execute_reply":"2025-05-24T19:25:34.837372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The target values `y` are currently of type object, which can cause issues with many machine learning models. We'll convert them to int to ensure compatibility. We'll also separate the target column from the feature set `X` to prepare for modeling.","metadata":{}},{"cell_type":"code","source":"X = df.drop('Loan_Status', axis= 1)\ny = df['Loan_Status']\ny = y.map({'N': 0, 'Y': 1})\ny.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:34.839315Z","iopub.execute_input":"2025-05-24T19:25:34.839584Z","iopub.status.idle":"2025-05-24T19:25:34.865677Z","shell.execute_reply.started":"2025-05-24T19:25:34.839559Z","shell.execute_reply":"2025-05-24T19:25:34.864120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating a new feature column for `TotalIncome`‚Äîthe combined income of the applicant and co-applicant‚Äîmay improve model performance. This can help capture overall financial capacity more effectively than using the individual income columns separately.","metadata":{}},{"cell_type":"code","source":"X['TotalIncome'] = X['ApplicantIncome'] + X['CoapplicantIncome'] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:34.867132Z","iopub.execute_input":"2025-05-24T19:25:34.867496Z","iopub.status.idle":"2025-05-24T19:25:34.887902Z","shell.execute_reply.started":"2025-05-24T19:25:34.867464Z","shell.execute_reply":"2025-05-24T19:25:34.886794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline model\nHere we create a pipeline. In the pipline we will use SimpleImputer to handle the NAN values in the numeric cols and catagory cols. For the numeric cols we will use strategy `mean` and for the catagorey cols we will use strategy `most_frequent`. We will then OneHotEncode the cat cols. \nWe will do all this in the pipline to make it easier to reproduce for the testing","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nnumeric_preprocessor = Pipeline(\n    steps =[\n        (\"imputation_mean\", SimpleImputer(missing_values=np.nan, strategy='mean')),\n        (\"scaler\", StandardScaler())\n    ]\n)\n\ncategorical_preprocessor = Pipeline(\n    steps = [\n        (\"imputation_frequent\", SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\npreprocessor = ColumnTransformer(\n    [\n        ('cat', categorical_preprocessor,['Dependents','Gender','Married', 'Self_Employed','Property_Area']),\n        ('num', numeric_preprocessor,['TotalIncome','Loan_Amount_Term','Credit_History'])\n    ]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:34.889714Z","iopub.execute_input":"2025-05-24T19:25:34.890218Z","iopub.status.idle":"2025-05-24T19:25:43.762852Z","shell.execute_reply.started":"2025-05-24T19:25:34.890184Z","shell.execute_reply":"2025-05-24T19:25:43.761873Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this section, we test several models and print their cross-validation scores to compare performance and identify the best baseline classifier.","metadata":{}},{"cell_type":"code","source":"models = {\n    'RandomForest': RandomForestClassifier(n_estimators=300, random_state=0),\n    'GradientBoost': GradientBoostingClassifier(random_state=0),\n    'XGBoost': XGBClassifier(n_estimators=400, learning_rate=0.05,\n                             max_depth=4, random_state=0, use_label_encoder=False, eval_metric='logloss'),\n    'LogisticRegression': LogisticRegression(max_iter=1000),\n    'LGBMClassifier': LGBMClassifier(n_estimators=400, learning_rate=0.05,\n                                     max_depth=4, random_state=0, verbose=-1)\n}\n\nfitted_pipes = {}\nfor name, model in models.items():\n    pipe = make_pipeline(preprocessor, model)\n    score = cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n    print(f'{name}: {score:.4f}')\n    fitted_pipes[name] = pipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:25:43.763751Z","iopub.execute_input":"2025-05-24T19:25:43.764453Z","iopub.status.idle":"2025-05-24T19:25:50.742695Z","shell.execute_reply.started":"2025-05-24T19:25:43.764425Z","shell.execute_reply":"2025-05-24T19:25:50.741672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Logistic Regression had the highest cross-validation score of 0.8127, so we‚Äôll focus on this model moving forward. Next, we'll compare its performance on the training and test sets to check for signs of overfitting or underfitting.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state = 0, stratify=y)\nfrom sklearn.metrics import accuracy_score, classification_report\n\nbest_pipe.fit(X_train, y_train)\n#create a funciton to compare train vs test from train_test_split\ndef train_to_test (fitted):\n\n    predictions1 = fitted.predict(X_train)\n    predictions2 = fitted.predict(X_test)\n\n    print('*******train********')\n    print(classification_report(y_train, predictions1))\n    print('*******test********')\n    print(classification_report(y_test, predictions2))\n    print(\"train Accuracy:\", accuracy_score(y_train, predictions1))\n    print(\"Test Accuracy:\", accuracy_score(y_test, predictions2))\ntrain_to_test(best_pipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:00:35.887565Z","iopub.execute_input":"2025-05-24T20:00:35.887898Z","iopub.status.idle":"2025-05-24T20:00:36.056363Z","shell.execute_reply.started":"2025-05-24T20:00:35.887876Z","shell.execute_reply":"2025-05-24T20:00:36.055599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The difference between both CV scores is very minimal, suggesting no overfitting. However the recall difference is quiet large, suggest false negatives are high (predicting ‚ÄúApproved‚Äù when it should be ‚ÄúNot Approved‚Äù). We can address this with `class_weight='balanced'` in the classifier  LogisticRegression to improve Class 0 recall.","metadata":{}},{"cell_type":"code","source":"new_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, class_weight='balanced'))\nnew_pipe.fit(X_train, y_train)\n\ntrain_to_test(new_pipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:26:34.715947Z","iopub.execute_input":"2025-05-24T19:26:34.716228Z","iopub.status.idle":"2025-05-24T19:26:34.897527Z","shell.execute_reply.started":"2025-05-24T19:26:34.716209Z","shell.execute_reply":"2025-05-24T19:26:34.896562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Adjusting for Class Imbalance with class_weight='balanced'\nBy using `class_weight='balanced'` in our Logistic Regression model, we aimed to improve the model‚Äôs ability to correctly identify the minority class (loan applications that should not be approved).\n\nWhile overall accuracy dropped slightly (from ~80% to ~79%), the recall for Class 0 (Not Approved) improved significantly on both the training and test sets. This is a meaningful trade-off: in a loan approval setting, false negatives (predicting Approved when the loan should actually be Declined) carry more risk than false positives. A false negative can lead to approving a loan for someone who doesn't meet the criteria, which may result in financial loss for the lender.\n\nImproving recall for the ‚ÄúNot Approved‚Äù class makes the model more cautious, which is generally preferred in high-stakes decisions like loan approvals. Accuracy alone doesn‚Äôt tell the full story‚Äîthis improvement shows the importance of balancing performance across classes.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning with GridSearchCV\nTo further improve our Logistic Regression model, we used `GridSearchCV` to perform an exhaustive search over a range of hyperparameters. This process helps identify the best combination of settings that maximize model performance based on cross-validation.\n\nWe tuned the following hyperparameters:\n\n* `C`: Inverse of regularization strength (tested values: 0.01, 0.1, 1, 10, 100)\n\n* `penalty`: Regularization type (l2 was used, which is standard for Logistic Regression)\n\n* `solver`: Algorithm to use in the optimization problem (lbfgs and liblinear)\n\n* `class_weight`: Adjusts for class imbalance (balanced was included to improve recall on the minority class)\n\nUsing 5-fold cross-validation, `GridSearchCV` evaluated each combination and returned the one with the highest average accuracy.\n\nThis step helps ensure we‚Äôre not just fitting a good model, but one that‚Äôs optimized for generalization to new, unseen data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'logisticregression__C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n    'logisticregression__penalty': ['l2'],             # Usually 'l2' for scikit-learn solver\n    'logisticregression__solver': ['lbfgs', 'liblinear'],\n    'logisticregression__class_weight': ['balanced']\n    \n}\n\ngrid = GridSearchCV(best_pipe, param_grid, cv=5, scoring='accuracy')\ngrid.fit(X_train, y_train)\n\nprint(f\"Best Params: {grid.best_params_}\")\nprint(f\"Best CV Score: {grid.best_score_:.4f}\")\n#print(classification_report(y_test, predictions2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:37:18.051832Z","iopub.execute_input":"2025-05-24T19:37:18.052138Z","iopub.status.idle":"2025-05-24T19:37:21.698586Z","shell.execute_reply.started":"2025-05-24T19:37:18.052119Z","shell.execute_reply":"2025-05-24T19:37:21.695785Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using the results from `GridSearchCV`, we now create a new Logistic Regression model with the best-performing hyperparameters. This step allows us to train and evaluate a model that is more fine-tuned to the data, which may improve overall performance and generalization on unseen data.","metadata":{}},{"cell_type":"code","source":"best_model = make_pipeline(preprocessor,LogisticRegression(C=.01, class_weight = 'balanced', penalty='l2', solver='lbfgs', max_iter=1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:38:03.936076Z","iopub.execute_input":"2025-05-24T19:38:03.936388Z","iopub.status.idle":"2025-05-24T19:38:03.941481Z","shell.execute_reply.started":"2025-05-24T19:38:03.936366Z","shell.execute_reply":"2025-05-24T19:38:03.940410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model.fit(X_train, y_train)\ntrain_to_test(best_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:38:12.397748Z","iopub.execute_input":"2025-05-24T19:38:12.398128Z","iopub.status.idle":"2025-05-24T19:38:12.470057Z","shell.execute_reply.started":"2025-05-24T19:38:12.398105Z","shell.execute_reply":"2025-05-24T19:38:12.469276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final Model Performance\n\nAfter tuning hyperparameters and rebuilding the Logistic Regression model, the results show that performance remains consistent with previous versions.\n\n- **Train Accuracy:** 80.04%\n- **Test Accuracy:** 80.49%\n\nThe model continues to perform well on both training and test sets, with minimal signs of overfitting. However, while accuracy is strong, the recall for class `0` (Loan Not Approved) remains low. This means the model still tends to predict more loan approvals than it should. In real-world loan approval scenarios, failing to identify high-risk applicants could carry financial consequences. Further improvements may require additional feature engineering, data balancing, or exploring alternative models.\n","metadata":{}},{"cell_type":"markdown","source":"### Create a Function to Plot a Confusion Matrix\n\nTo better visualize model performance, we'll define a reusable function to plot the confusion matrix. This function takes in the model, feature set, and true labels, and displays the confusion matrix with labeled axes and color shading for better clarity.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef plot_confusion_matrix(y,y_predict):\n    \"this function plots the confusion matrix\"\n    \n\n    cm = confusion_matrix(y, y_predict)\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    ax.set_title('Confusion Matrix'); \n    ax.xaxis.set_ticklabels(['not approved', 'approved']); ax.yaxis.set_ticklabels(['not approved', 'approved'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:03:02.883585Z","iopub.execute_input":"2025-05-24T20:03:02.883989Z","iopub.status.idle":"2025-05-24T20:03:03.358830Z","shell.execute_reply.started":"2025-05-24T20:03:02.883965Z","shell.execute_reply":"2025-05-24T20:03:03.357506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yhat = best_model.predict(X_test)\nplot_confusion_matrix(y_test,yhat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:08:19.862571Z","iopub.execute_input":"2025-05-24T20:08:19.862987Z","iopub.status.idle":"2025-05-24T20:08:20.114475Z","shell.execute_reply.started":"2025-05-24T20:08:19.862963Z","shell.execute_reply":"2025-05-24T20:08:20.113352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Get predicted probabilities","metadata":{}},{"cell_type":"markdown","source":"### Plot Precision-Recall vs Threshold\n\nTo understand how different probability thresholds affect precision and recall, we plot both metrics against various threshold values. This helps us choose an optimal threshold for classification decisions, especially in imbalanced datasets where accuracy alone may be misleading.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprobs = best_model.predict_proba(X_test)[:, 1]\nprecision, recall, pr_thresholds = precision_recall_curve(y_test, probs)\n\nplt.figure(figsize=(8, 5))\nplt.plot(pr_thresholds, precision[:-1], label='Precision')\nplt.plot(pr_thresholds, recall[:-1], label='Recall')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.title('Precision-Recall vs Threshold')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:29:34.929196Z","iopub.execute_input":"2025-05-24T20:29:34.929494Z","iopub.status.idle":"2025-05-24T20:29:35.133939Z","shell.execute_reply.started":"2025-05-24T20:29:34.929472Z","shell.execute_reply":"2025-05-24T20:29:35.132930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate Model at a Custom Threshold\n\nInstead of using the default 0.5 threshold, we manually set a custom threshold of 0.52 to convert predicted probabilities into class labels. This allows us to fine-tune the trade-off between precision and recall based on the problem context. After applying the threshold, we evaluate the model using a classification report and confusion matrix.\n","metadata":{}},{"cell_type":"code","source":"y_pred_custom = (probs > .52).astype(int)\nprint(classification_report(y_test, y_pred_custom))\nplot_confusion_matrix(y_test, y_pred_custom)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:31:35.696355Z","iopub.execute_input":"2025-05-24T20:31:35.696686Z","iopub.status.idle":"2025-05-24T20:31:35.931388Z","shell.execute_reply.started":"2025-05-24T20:31:35.696664Z","shell.execute_reply":"2025-05-24T20:31:35.930602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Results at Custom Threshold (0.52)\n\nAdjusting the classification threshold to 0.52 led to a more balanced performance between classes. Class 0 (Not Approved) recall improved compared to the default threshold, which helps reduce false positives (i.e., predicting \"Approved\" when it shouldn't be). This is especially important in loan approval settings where misclassifying unqualified applicants can lead to financial risk.\n\n| Metric         | Class 0 | Class 1 |\n|----------------|---------|---------|\n| Precision      | 0.78    | 0.80    |\n| Recall         | 0.47    | 0.94    |\n| F1-score       | 0.59    | 0.86    |\n| **Accuracy**   | **0.80** overall\n\nThe custom threshold offers a practical improvement in recall for the minority class, which could better support business goals depending on the use case.\n","metadata":{}},{"cell_type":"markdown","source":"# Feature Importance Analysis","metadata":{}},{"cell_type":"markdown","source":"### Top 10 Most Influential Features in Logistic Regression\n\nTo better understand which features had the greatest impact on the model's predictions, we extracted the coefficients from the trained Logistic Regression model. Using the pipeline's preprocessor, we matched each coefficient to its corresponding feature.\n\nBelow are the top 10 features sorted by absolute coefficient magnitude, representing the most influential inputs in predicting loan approval","metadata":{}},{"cell_type":"code","source":"best_pipe = fitted_pipes['LogisticRegression']\n# Get the final Logistic Regression model\nlog_reg = best_pipe['logisticregression']\n\n# Get feature names from the preprocessor\n# This works if you're using ColumnTransformer with named transformers\nfeature_names = best_pipe.named_steps['columntransformer'].get_feature_names_out()\n\n# Pair each feature name with its coefficient\ncoefficients = pd.Series(log_reg.coef_[0], index=feature_names)\n\n# Sort by importance\ncoefficients.abs().sort_values(ascending=False).head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:26:27.878352Z","iopub.execute_input":"2025-05-24T19:26:27.878662Z","iopub.status.idle":"2025-05-24T19:26:27.888956Z","shell.execute_reply.started":"2025-05-24T19:26:27.878643Z","shell.execute_reply":"2025-05-24T19:26:27.887948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"üîç Feature Importance Summary\nTo better understand what influenced our model's predictions, we extracted the coefficients from the final Logistic Regression model. Using the names from our ColumnTransformer, we matched each feature to its corresponding weight.\n\nThe results showed that the most important feature by far was `Credit_History`, followed by indicators for property location (`Property_Area_Semiurban, Property_Area_Rural`) and dependent/marital status (`Dependents_1`, `Married_Yes`, etc.).\n\nThese coefficients reflect the magnitude of each feature‚Äôs impact on the model‚Äôs decision ‚Äî not whether it increased or decreased the chance of loan approval, but how strongly it influenced the prediction. This helped guide our feature reduction and model refinement in the next steps.","metadata":{}},{"cell_type":"markdown","source":"### Feature Reduction\n\nTo simplify the model and potentially improve generalization, we will perform feature reduction by selecting the top 10 most influential features based on the absolute values of the Logistic Regression coefficients. \n\nUsing these features, we‚Äôll rebuild the pipeline and retrain the model to evaluate if performance can be maintained or improved with fewer inputs.","metadata":{}},{"cell_type":"code","source":"important_numeric = Pipeline(\n    steps =[\n        (\"imputation_mean\", SimpleImputer(missing_values=np.nan, strategy='mean')),\n        (\"scaler\", StandardScaler())\n    ]\n)\n\nimportant_cat = Pipeline(\n    steps = [\n        (\"imputation_frequent\", SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\npreprocessor_important = ColumnTransformer(\n    [\n        ('cat', important_cat,['Dependents','Married','Property_Area']),\n        ('num', important_numeric,['TotalIncome','Credit_History'])\n    ]\n)\n\n# New pipeline\nreduced_pipe = Pipeline([\n    ('preprocessor', preprocessor_important),\n    ('logisticregression', LogisticRegression(C=.01, class_weight = 'balanced', penalty='l2', solver='lbfgs', max_iter=1000))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:04:26.689458Z","iopub.execute_input":"2025-05-24T21:04:26.689807Z","iopub.status.idle":"2025-05-24T21:04:26.695671Z","shell.execute_reply.started":"2025-05-24T21:04:26.689784Z","shell.execute_reply":"2025-05-24T21:04:26.694619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduced_pipe.fit(X_train, y_train)\ntrain_to_test(reduced_pipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:00:54.105119Z","iopub.execute_input":"2025-05-24T21:00:54.105452Z","iopub.status.idle":"2025-05-24T21:00:54.175590Z","shell.execute_reply.started":"2025-05-24T21:00:54.105430Z","shell.execute_reply":"2025-05-24T21:00:54.174897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yhat = reduced_pipe.predict(X_test)\nplot_confusion_matrix(y_test,yhat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:02:59.852474Z","iopub.execute_input":"2025-05-24T21:02:59.853316Z","iopub.status.idle":"2025-05-24T21:03:00.113946Z","shell.execute_reply.started":"2025-05-24T21:02:59.853274Z","shell.execute_reply":"2025-05-24T21:03:00.112850Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insights\n\nAfter reducing the feature set to only the most impactful variables, the model's performance remained nearly identical to the full-feature version. The confusion matrix confirms that the balance between false positives and false negatives was maintained. \n\nThis suggests that the core predictive power lies in a small number of features, which makes the model:\n- Easier to interpret\n- Faster to train and run\n- Less prone to overfitting\n\nOverall, simplifying the model led to a more efficient pipeline without sacrificing accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"### Comparing Full vs Reduced Model Performance\n\nTo evaluate whether reducing the number of features impacts model performance, we compare the classification metrics‚Äîaccuracy, precision, recall, and F1-score‚Äîof the full model and the reduced model.\n\nThis comparison helps us determine if a simpler model (fewer features) performs just as well as the more complex version. If the reduced model performs similarly, we can justify using it due to its increased interpretability, faster inference time, and potentially lower risk of overfitting.\n\nWe define a helper function `get_metrics()` to calculate the key metrics and display them side-by-side for easy comparison.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\ndef get_metrics(y_true, y_pred):\n    return {\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Precision\": precision_score(y_true, y_pred),\n        \"Recall\": recall_score(y_true, y_pred),\n        \"F1\": f1_score(y_true, y_pred)\n    }\n\nresults = pd.DataFrame({\n    \"Full Model\": get_metrics(y_test, best_model.predict(X_test)),\n    \"Reduced Model\": get_metrics(y_test, reduced_pipe.predict(X_test))\n})\n\nresults.T  # Transpose for readability\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:15:22.547984Z","iopub.execute_input":"2025-05-24T21:15:22.548320Z","iopub.status.idle":"2025-05-24T21:15:22.600198Z","shell.execute_reply.started":"2025-05-24T21:15:22.548298Z","shell.execute_reply":"2025-05-24T21:15:22.599275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîç Model Comparison Summary\n\nAfter selecting the top features based on logistic regression coefficients, we trained a reduced model using only those important features. We then compared its performance to the original full-feature model.\n\n**Key Result:**  \nThe reduced model achieved the **same performance** as the original model on both the training and test sets:\n\n- **Train Accuracy:** 80.0%\n- **Test Accuracy:** 80.5%\n- **Precision, Recall, F1:** Nearly identical across classes\n\n**What this tells us:**\n- Most of the model‚Äôs predictive power came from a small subset of features.\n- Removing less informative features led to a simpler, more interpretable model without sacrificing performance.\n- This reinforces that a leaner model can still be highly effective ‚Äî a useful insight for production use or stakeholder explanation.\n\nNext steps could include using this leaner model as the final version, or trying regularization tuning or ensemble methods for incremental gains.\n","metadata":{}}]}